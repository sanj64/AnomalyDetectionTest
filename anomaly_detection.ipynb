{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8S9EMswy54aZ"
      },
      "source": [
        "### Anomaly Detection with Isolation Forest\n",
        "\n",
        "**Goal:** To detect unusual patterns (anomalies) in synthetic time-series data using the Isolation Forest algorithm.\n",
        "\n",
        "**What is Anomaly Detection?**\n",
        "Anomaly detection, also known as outlier detection, is the process of identifying data points that deviate significantly from the majority of the data. These \"anomalies\" can indicate critical events, errors, or interesting insights. In time-series data, an anomaly might be an unusually high sensor reading, a sudden drop in website traffic, or a burst of network activity.\n",
        "\n",
        "**Why Isolation Forest?**\n",
        "Isolation Forest is an unsupervised machine learning algorithm particularly well-suited for anomaly detection. Its core idea is that anomalies are \"few and different\" and thus easier to isolate than normal data points. It does this by randomly partitioning data and observing how many splits it takes to isolate a data point. Anomalies typically require fewer splits to be isolated."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Mb8kTWwi54ad"
      },
      "outputs": [],
      "source": [
        "# Cell 1: Import necessary libraries\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.ensemble import IsolationForest\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "import random # For injecting anomalies"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5QZJ-aEA54af"
      },
      "source": [
        "**Explanation for Cell 1:**\n",
        "* `numpy` (as `np`): Fundamental package for numerical computation in Python, especially for arrays.\n",
        "* `pandas` (as `pd`): Used for data manipulation and analysis, especially with DataFrames.\n",
        "* `matplotlib.pyplot` (as `plt`): For creating static, interactive, and animated visualizations in Python.\n",
        "* `IsolationForest` from `sklearn.ensemble`: The core machine learning model we'll use for anomaly detection.\n",
        "* `StandardScaler` from `sklearn.preprocessing`: Used to standardize features by removing the mean and scaling to unit variance. While Isolation Forest is less sensitive to scaling than some other algorithms, it's good practice, especially if you were to compare with other models later.\n",
        "* `random`: Will be used to inject random anomalies into our synthetic data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Jgudqs9J54ag"
      },
      "outputs": [],
      "source": [
        "# Cell 2: Generate Synthetic Time-Series Data\n",
        "\n",
        "# Set a random seed for reproducibility\n",
        "np.random.seed(42)\n",
        "random.seed(42)\n",
        "\n",
        "# --- Generate Normal Data ---\n",
        "# Number of data points\n",
        "n_samples = 1000\n",
        "\n",
        "# Create a time index\n",
        "dates = pd.date_range(start='2023-01-01', periods=n_samples, freq='H')\n",
        "\n",
        "# Simulate a \"normal\" sensor reading with a slight trend and seasonality\n",
        "# Base value with some daily fluctuation\n",
        "normal_data = np.sin(np.linspace(0, 50, n_samples)) * 5 + np.linspace(0, 20, n_samples) + np.random.normal(0, 1, n_samples)\n",
        "\n",
        "# Create a DataFrame\n",
        "df = pd.DataFrame({'timestamp': dates, 'value': normal_data})\n",
        "\n",
        "# --- Inject Anomalies ---\n",
        "# Inject some sudden spikes (point anomalies)\n",
        "num_spikes = 10\n",
        "spike_indices = random.sample(range(n_samples), num_spikes)\n",
        "for idx in spike_indices:\n",
        "    df.loc[idx, 'value'] += np.random.uniform(20, 50) # Add a large random value\n",
        "\n",
        "# Inject some sudden drops (point anomalies)\n",
        "num_drops = 5\n",
        "drop_indices = random.sample(range(n_samples), num_drops)\n",
        "for idx in drop_indices:\n",
        "    df.loc[idx, 'value'] -= np.random.uniform(20, 50) # Subtract a large random value\n",
        "\n",
        "# Inject a short \"contextual anomaly\" (a sustained unusual period)\n",
        "contextual_start = 500\n",
        "contextual_end = 530\n",
        "df.loc[contextual_start:contextual_end, 'value'] = np.random.normal(70, 5, contextual_end - contextual_start + 1)\n",
        "\n",
        "print(\"Synthetic data generated with normal patterns and injected anomalies.\")\n",
        "print(df.head())\n",
        "print(\"\\nDataFrame Info:\")\n",
        "df.info()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cPQgHPM_54aj"
      },
      "source": [
        "**Explanation for Cell 2:**\n",
        "* **Reproducibility:** `np.random.seed()` and `random.seed()` ensure that every time you run this notebook, you get the exact same \"random\" data, which is crucial for debugging and and comparing results.\n",
        "* **Normal Data Generation:**\n",
        "    * We create a `pd.date_range` to simulate hourly sensor readings over a period.\n",
        "    * The `normal_data` is generated using a combination of:\n",
        "        * `np.sin()`: To introduce a cyclical (seasonal) pattern.\n",
        "        * `np.linspace()`: To create a gradual increasing trend.\n",
        "        * `np.random.normal()`: To add realistic noise.\n",
        "    * This combination gives us a baseline time-series that looks somewhat natural.\n",
        "* **Anomaly Injection:**\n",
        "    * **Spikes/Drops:** We randomly select indices and add/subtract large values to simulate sudden, isolated anomalous events. These are **point anomalies**.\n",
        "    * **Contextual Anomaly:** We select a continuous block of time and replace the data with values that are unusual for that period (e.g., a sustained higher reading), even if individual points aren't extreme. This demonstrates a **contextual anomaly**.\n",
        "* **DataFrame Creation:** The generated data is stored in a Pandas DataFrame, which is a standard way to handle tabular data in Python.\n",
        "* **`df.head()` and `df.info()`:** These are used to inspect the first few rows of the data and get a summary of its structure (data types, non-null counts)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LEDVryLK54ak"
      },
      "outputs": [],
      "source": [
        "# Cell 3: Visualize the Raw Time-Series Data\n",
        "\n",
        "plt.figure(figsize=(15, 7))\n",
        "plt.plot(df['timestamp'], df['value'], label='Sensor Value', alpha=0.8)\n",
        "plt.title('Synthetic Time-Series Data with Injected Anomalies')\n",
        "plt.xlabel('Time')\n",
        "plt.ylabel('Value')\n",
        "plt.grid(True)\n",
        "plt.legend()\n",
        "plt.show()\n",
        "\n",
        "print(\"Notice the sudden spikes, drops, and the sustained unusual period. These are our injected anomalies.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jQyP-6AO54al"
      },
      "source": [
        "**Explanation for Cell 3:**\n",
        "* This cell simply plots the `value` column against the `timestamp` to visually inspect the data.\n",
        "* It's important to visualize your data to understand its patterns and to confirm that the anomalies you injected (or expect to find) are actually visible. This helps in intuition building."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "m_Mj4gCo54am"
      },
      "outputs": [],
      "source": [
        "# Cell 4: Prepare Data for Isolation Forest\n",
        "\n",
        "# Isolation Forest works best on numerical features.\n",
        "# We will focus on the 'value' column for anomaly detection.\n",
        "# If you had multiple features (e.g., temperature, pressure, humidity),\n",
        "# you would select all of them here.\n",
        "data_for_model = df[['value']]\n",
        "\n",
        "# Optional: Scale the data. While Isolation Forest is not highly sensitive to scaling,\n",
        "# it can sometimes help with performance and is generally good practice.\n",
        "scaler = StandardScaler()\n",
        "scaled_data = scaler.fit_transform(data_for_model)\n",
        "\n",
        "print(f\"Original data shape: {data_for_model.shape}\")\n",
        "print(f\"Scaled data shape: {scaled_data.shape}\")\n",
        "print(\"Data prepared for Isolation Forest.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1WiMoENJ54an"
      },
      "source": [
        "**Explanation for Cell 4:**\n",
        "* **Feature Selection:** We select only the `'value'` column as our feature for anomaly detection. In a real-world scenario, you might have multiple sensor readings or metrics that collectively define \"normal\" behavior, and you would pass all of them to the model.\n",
        "* **`StandardScaler`:**\n",
        "    * `fit_transform()`: This method calculates the mean and standard deviation of the `data_for_model` (`fit`) and then applies the scaling transformation (`transform`). This results in data with a mean of 0 and a standard deviation of 1, which can make optimization easier for some algorithms and prevent features with larger scales from dominating."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sGq_yoCe54an"
      },
      "outputs": [],
      "source": [
        "# Cell 5: Train the Isolation Forest Model\n",
        "\n",
        "# Initialize the Isolation Forest model\n",
        "# key parameters:\n",
        "# n_estimators: The number of trees in the forest. More trees generally lead to more robust results.\n",
        "# contamination: The expected proportion of outliers in the data. This is an important parameter\n",
        "#                as it defines the threshold for anomaly scores. If 'auto', the threshold is\n",
        "#                determined by the original paper. We're setting it explicitly based on our\n",
        "#                injected anomalies (10 spikes + 5 drops + 30 contextual points = ~45/1000 = 0.045)\n",
        "#                Let's use 0.05 (5%) as a reasonable estimate.\n",
        "# random_state: For reproducibility of the model's training process.\n",
        "model = IsolationForest(n_estimators=100, contamination=0.05, random_state=42)\n",
        "\n",
        "# Fit the model to the scaled data\n",
        "# The fit method 'learns' the normal behavior from the data.\n",
        "model.fit(scaled_data)\n",
        "\n",
        "print(\"Isolation Forest model trained successfully.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sPO5I3kd54ao"
      },
      "source": [
        "**Explanation for Cell 5:**\n",
        "* **`IsolationForest` Initialization:**\n",
        "    * `n_estimators=100`: We're building 100 \"isolation trees.\" More trees generally improve accuracy but increase computation time.\n",
        "    * `contamination=0.05`: This is a crucial hyperparameter. It's our *estimate* of the proportion of anomalies in the dataset. Isolation Forest uses this to set a decision threshold: it will identify the top `contamination` percentage of data points with the lowest anomaly scores (most anomalous) as outliers. If you don't know the contamination, you can leave it as `'auto'` or experiment with different values.\n",
        "    * `random_state=42`: Again, for reproducibility of the model's internal random processes.\n",
        "* **`model.fit(scaled_data)`:** This is where the magic happens. The model learns the underlying structure of the normal data by building the isolation trees."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3RrOf2Ox54ap"
      },
      "outputs": [],
      "source": [
        "# Cell 6: Predict Anomalies and Get Anomaly Scores\n",
        "\n",
        "# `predict` method returns -1 for anomalies and 1 for normal points\n",
        "df['anomaly_prediction'] = model.predict(scaled_data)\n",
        "\n",
        "# `decision_function` returns the anomaly scores.\n",
        "# Lower scores indicate a higher likelihood of being an anomaly.\n",
        "df['anomaly_score'] = model.decision_function(scaled_data)\n",
        "\n",
        "# Convert predictions to a more intuitive label\n",
        "# -1 -> Anomaly, 1 -> Normal\n",
        "df['is_anomaly'] = df['anomaly_prediction'].apply(lambda x: 'Anomaly' if x == -1 else 'Normal')\n",
        "\n",
        "print(\"Anomaly predictions and scores generated.\")\n",
        "print(df.head())\n",
        "print(\"\\nAnomaly counts:\")\n",
        "print(df['is_anomaly'].value_counts())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hTa-msPK54ap"
      },
      "source": [
        "**Explanation for Cell 6:**\n",
        "* **`model.predict(scaled_data)`:** This method applies the trained Isolation Forest to your data and returns a prediction for each data point:\n",
        "    * `-1`: Indicates an anomaly.\n",
        "    * `1`: Indicates a normal data point.\n",
        "* **`model.decision_function(scaled_data)`:** This method returns the raw anomaly score for each data point.\n",
        "    * **Interpretation:** For Isolation Forest, a *lower* (more negative) score means a higher likelihood of being an anomaly. Data points that are easier to isolate (requiring fewer splits) will have lower scores.\n",
        "* **`df['is_anomaly']`:** We create a more human-readable column by mapping the `-1` and `1` predictions to 'Anomaly' and 'Normal' strings.\n",
        "* **`value_counts()`:** Shows how many data points were classified as normal and how many as anomalous, based on the `contamination` parameter set during model initialization."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mH5XpsSR54aq"
      },
      "outputs": [],
      "source": [
        "# Cell 7: Visualize Detected Anomalies\n",
        "\n",
        "plt.figure(figsize=(18, 8))\n",
        "\n",
        "# Plot normal data points\n",
        "normal_points = df[df['is_anomaly'] == 'Normal']\n",
        "plt.plot(normal_points['timestamp'], normal_points['value'], 'b.', markersize=8, label='Normal Data', alpha=0.6)\n",
        "\n",
        "# Plot anomalies\n",
        "anomaly_points = df[df['is_anomaly'] == 'Anomaly']\n",
        "plt.plot(anomaly_points['timestamp'], anomaly_points['value'], 'ro', markersize=6, label='Detected Anomaly', alpha=0.9)\n",
        "\n",
        "plt.title('Time-Series Anomaly Detection with Isolation Forest')\n",
        "plt.xlabel('Time')\n",
        "plt.ylabel('Value')\n",
        "plt.grid(True)\n",
        "plt.legend()\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(\"The red circles indicate data points detected as anomalies.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_E_PLvXI54aq"
      },
      "source": [
        "**Explanation for Cell 7:**\n",
        "* This cell visualizes the results of our anomaly detection.\n",
        "* It separates the DataFrame into two parts: `normal_points` and `anomaly_points` based on the `is_anomaly` column.\n",
        "* Normal points are plotted as blue dots, and detected anomalies are plotted as distinct red circles. This visual representation makes it very clear where the anomalies were detected and how they align with the injected anomalies."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZD4U7YTy54ar"
      },
      "outputs": [],
      "source": [
        "# Cell 8: Explore Anomaly Scores (Optional but Recommended)\n",
        "\n",
        "plt.figure(figsize=(15, 6))\n",
        "plt.hist(df['anomaly_score'], bins=50, density=True, alpha=0.7, color='c')\n",
        "plt.title('Distribution of Anomaly Scores')\n",
        "plt.xlabel('Anomaly Score')\n",
        "plt.ylabel('Density')\n",
        "plt.grid(True)\n",
        "plt.show()\n",
        "\n",
        "# You can also look at the most anomalous points\n",
        "print(\"\\nTop 10 most anomalous points (lowest scores):\")\n",
        "print(df.sort_values(by='anomaly_score').head(10))\n",
        "\n",
        "print(\"\\nInterpretation of scores: Lower (more negative) scores indicate higher likelihood of being an anomaly.\")\n",
        "print(\"The 'contamination' parameter in IsolationForest sets the threshold internally.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Iuqp_CML54ar"
      },
      "source": [
        "**Explanation for Cell 8:**\n",
        "* **Histogram of Scores:** This plot shows the distribution of anomaly scores. You'll typically see a cluster of scores around a higher value (for normal points) and a tail extending to lower (more negative) values where the anomalies lie.\n",
        "* **Top Anomalous Points:** By sorting the DataFrame by `anomaly_score` in ascending order, you can easily identify the data points that the model considers most anomalous. This is useful for further investigation."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ipk26Nta54ar"
      },
      "source": [
        "### Short README/Summary\n",
        "\n",
        "This Jupyter Notebook demonstrates a simple **Anomaly Detection** task using the **Isolation Forest** algorithm.\n",
        "\n",
        "**1. Data Generation:**\n",
        "  * Synthetic time-series data is created with a gentle trend and seasonality to mimic real-world sensor data.\n",
        "  * Specific anomalies (sudden spikes, drops, and a sustained unusual period) are deliberately injected into this data to serve as ground truth for our detection.\n",
        "\n",
        "**2. Data Preparation:**\n",
        "  * The `value` column (representing our time-series measurement) is selected as the feature for the model.\n",
        "  * The data is optionally scaled using `StandardScaler` to ensure all features contribute equally, although Isolation Forest is robust to unscaled data.\n",
        "\n",
        "**3. Model Training:**\n",
        "   * An `IsolationForest` model from `scikit-learn` is initialized.\n",
        "   * Key parameters like `n_estimators` (number of trees) and `contamination` (estimated proportion of anomalies) are set.\n",
        "   * The model is trained on the prepared data. Being an unsupervised algorithm, it learns patterns of \"normality\" without needing explicit anomaly labels.\n",
        "\n",
        "**4. Anomaly Prediction & Scoring:**\n",
        "   * The trained model `predicts` whether each data point is an anomaly (`-1`) or normal (`1`).\n",
        "   * It also provides `decision_function` scores, where lower (more negative) scores indicate a higher likelihood of being an anomaly.\n",
        "\n",
        "**5. Visualization:**\n",
        "   * The original time-series data is plotted, with detected anomalies highlighted in a distinct color (red circles), making the results visually intuitive.\n",
        "   * A histogram of anomaly scores is provided to understand the distribution of anomaly likelihoods.\n",
        "\n",
        "**How Isolation Forest Works:**\n",
        "  \n",
        "Isolation Forest builds multiple \"isolation trees\" by randomly selecting a feature and then a random split value for that feature. This process is repeated recursively. Anomalies, being \"different\" from the majority, tend to be isolated closer to the root of these trees (requiring fewer splits), resulting in shorter \"path lengths.\" Normal data points, being more densely packed, require more splits to be isolated, leading to longer path lengths. The anomaly score is based on these path lengths: shorter paths indicate higher anomaly likelihood."
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python [conda env:base] *",
      "language": "python",
      "name": "conda-base-py"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.7"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}